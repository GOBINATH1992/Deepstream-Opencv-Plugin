
*****************************************************************************

*****************************************************************************
                     deepstream-opencv-plugin-test
                             README
*****************************************************************************

===============================================================================
1. Prerequisites:
===============================================================================

Please follow instructions in the apps/sample_apps/deepstream-app/README on how
to install the prequisites for Deepstream SDK, the DeepStream SDK itself and the
apps.

You must have the following development packages installed
    GStreamer-1.0
    GStreamer-1.0 Base Plugins
    GStreamer-1.0 gstrtspserver
    X11 client-side library

To install these packages, execute the following command:
   sudo apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev \
   libgstrtspserver-1.0-dev libx11-dev

This example can be configured to use either the nvinfer or the nvinferserver
element for inference.
If nvinferserver is selected, the Triton Inference Server is used for inference
processing. In this case, the example needs to be run inside the
DeepStream-Triton docker container. Please refer
samples/configs/deepstream-app-triton/README for the steps to download the
container image and setup model repository.

===============================================================================
2. Purpose:
===============================================================================

This document shall describe about the sample deepstream_test_opencv_plugin application.

It is meant to demonstrate how cuda frames acquired from outside DeepStream
can be fed to a DeepStream pipeline Using opencv. It also demostrates how metadata can be accessed
via appsink and used outside deepstream.

===============================================================================
3. To compile:
===============================================================================

  $ Set CUDA_VER in the MakeFile as per platform.
      For x86, CUDA_VER=12.1
  $ sudo make (sudo not required in case of docker containers)

Note: This application is validated on x86.

===============================================================================
4. Usage:
===============================================================================


To run the application with raw video stream:
  $ ./deepstream_test_opencv_plugin <video stream> <width of stream>
      <height of stream> <expected FPS of stream> <format of stream (RGBA)>
  e.g.
  $ ./deepstream_test_opencv_plugin /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4 1280 720 30 RGBA

Use option "-t inferserver" to select nvinferserver as the inference plugin
  $ ./deepstream_test_opencv_plugin -t inferserver <Raw video stream (example: YUV)> <width of stream>
      <height of stream> <expected FPS of stream> <format of stream (example: I420, NV12, RGBA)>
  e.g.
  $ ./deepstream_test_opencv_plugin -t inferserver /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4 1280 720 30 RGBA
    
This sample uses Opencv Plugin to push raw video frames into the pipeline,

A cuda memory block is created. Single primary inferencing is used here. Then the buffers are sent via a tee to regular video rendering sink and
appsink. Appsink extracts buffer from sample and then obtains metadata information from it.


